% A rule of thumb: at least 20 references, but no more than 50. 30-35 is often the ideal. 


@article{Laender:2002:BSW:565117.565137,
	author = {Laender, Alberto H. F. and Ribeiro-Neto, Berthier A. and da Silva, Altigran S. and Teixeira, Juliana S.},
	title = {A brief survey of web data extraction tools},
	journal = {SIGMOD Rec.},
	issue_date = {June 2002},
	volume = {31},
	number = {2},
	month = jun,
	year = {2002},
	issn = {0163-5808},
	pages = {84--93},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/565117.565137},
	doi = {10.1145/565117.565137},
	acmid = {565137},
	publisher = {ACM},
	address = {new york, ny, usa},
} 

@article{Chang:2006:SWI:1159162.1159300,
	author = {Chang, Chia-Hui and Kayed, Mohammed and Girgis, Moheb Ramzy and Shaalan, Khaled F.},
	title = {A Survey of Web Information Extraction Systems},
	journal = {IEEE Trans. on Knowl. and Data Eng.},
	issue_date = {October 2006},
	volume = {18},
	number = {10},
	month = oct,
	year = {2006},
	issn = {1041-4347},
	pages = {1411--1428},
	numpages = {18},
	url = {http://dx.doi.org/10.1109/TKDE.2006.152},
	doi = {10.1109/TKDE.2006.152},
	acmid = {1159300},
	publisher = {IEEE Educational Activities Department},
	address = {Piscataway, NJ, USA},
	keywords = {Information extraction, Information extraction, Web mining, wrapper, wrapper induction., Web mining, wrapper, wrapper induction.},
} 

@inproceedings{Kowalkiewicz:2006:RWC:1135777.1135928,
	author = {Kowalkiewicz, Marek and Orlowska, Maria E. and Kaczmarek, Tomasz and Abramowicz, Witold},
	title = {Robust web content extraction},
	booktitle = {Proceedings of the 15th international conference on World Wide Web},
	series = {WWW '06},
	year = {2006},
	isbn = {1-59593-323-9},
	location = {Edinburgh, Scotland},
	pages = {887--888},
	numpages = {2},
	url = {http://doi.acm.org/10.1145/1135777.1135928},
	doi = {10.1145/1135777.1135928},
	acmid = {1135928},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {content extraction, evaluation, robustness, wrappers},
} 

@article{DBLP:journals/pvldb/ParameswaranDGR11,
  author    = {Aditya G. Parameswaran and
               Nilesh N. Dalvi and
               Hector Garcia-Molina and
               Rajeev Rastogi},
  title     = {Optimal Schemes for Robust Web Extraction},
  journal   = {PVLDB},
  volume    = {4},
  number    = {11},
  year      = {2011},
  pages     = {980-991},
  ee        = {http://www.vldb.org/pvldb/vol4/p980-parameswaran.pdf},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@inproceedings{DBLP:conf/wism/LiuWYL12,
  author    = {Donglan Liu and
               Xinjun Wang and
               Zhongmin Yan and
               Qiuyan Li},
  title     = {Robust Web Data Extraction: A Novel Approach Based on Minimum
               Cost Script Edit Model},
  booktitle = {WISM},
  year      = {2012},
  pages     = {497-509},
  ee        = {http://dx.doi.org/10.1007/978-3-642-33469-6_62},
  crossref  = {DBLP:conf/wism/2012},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{Myllymaki02robustweb,
    author = {Jussi Myllymaki and Jared Jackson},
    title = {Robust Web Data Extraction with XML Path Expressions},
    journal = {IBM Research Report},
    year = {2002}
}

@inproceedings{DBLP:conf/icde/GulhaneMMRRSSTT11,
  author    = {Pankaj Gulhane and
               Amit Madaan and
               Rupesh R. Mehta and
               Jeyashankher Ramamirtham and
               Rajeev Rastogi and
               Sandeepkumar Satpal and
               Srinivasan H. Sengamedu and
               Ashwin Tengli and
               Charu Tiwari},
  title     = {Web-scale information extraction with vertex},
  booktitle = {ICDE},
  year      = {2011},
  pages     = {1209-1220},
  ee        = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2011.5767842},
  crossref  = {DBLP:conf/icde/2011},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@inproceedings{Thomsen:2012:WWS:2364120.2364156,
 author = {Thomsen, Jakob G. and Ernst, Erik and Brabrand, Claus and Schwartzbach, Michael},
 title = {WebSelF: a web scraping framework},
 booktitle = {Proceedings of the 12th international conference on Web Engineering},
 series = {ICWE'12},
 year = {2012},
 isbn = {978-3-642-31752-1},
 location = {Berlin, Germany},
 pages = {347--361},
 numpages = {15},
 url = {http://dx.doi.org/10.1007/978-3-642-31753-8_28},
 doi = {10.1007/978-3-642-31753-8_28},
 acmid = {2364156},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 

@inproceedings{DBLP:conf/cikm/ZhengSWG09,
  author    = {Shuyi Zheng and
               Ruihua Song and
               Ji-Rong Wen and
               C. Lee Giles},
  title     = {Efficient record-level wrapper induction},
  booktitle = {CIKM},
  year      = {2009},
  pages     = {47-56},
  ee        = {http://doi.acm.org/10.1145/1645953.1645962},
  crossref  = {DBLP:conf/cikm/2009},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@proceedings{DBLP:conf/cikm/2009,
  editor    = {David Wai-Lok Cheung and
               Il-Yeol Song and
               Wesley W. Chu and
               Xiaohua Hu and
               Jimmy J. Lin},
  title     = {Proceedings of the 18th ACM Conference on Information and
               Knowledge Management, CIKM 2009, Hong Kong, China, November
               2-6, 2009},
  booktitle = {CIKM},
  publisher = {ACM},
  year      = {2009},
  isbn      = {978-1-60558-512-3},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@inproceedings{Chidlovskii:2006:DES:1142473.1142555,
 author = {Chidlovskii, Boris and Roustant, Bruno and Brette, Marc},
 title = {Documentum ECI Self-repairing Wrappers: Performance Analysis},
 booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
 series = {SIGMOD '06},
 year = {2006},
 isbn = {1-59593-434-0},
 location = {Chicago, IL, USA},
 pages = {708--717},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1142473.1142555},
 doi = {10.1145/1142473.1142555},
 acmid = {1142555},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content integration, web wrappers, wrapper maintenance},
} 

@article{signorini2005a,
  title = {The indexable web is more than 11.5 billion pages},
  publisher = {Association for Computing Machinery},
  author = {Signorini, A. and Gulli, A.},
  journal = {14th International World Wide Web Conference, WWW2005},
  pages = {902-903},
  year = {2005},
  isbn = {1595930515, 9781595930514},
  abstract = {In this short paper we estimate the size of the public indexable web at 11.5 billion pages. We also estimate the overlap and the index size of Google, MSN, Ask/Teoma and Yahoo!},
  doi = {10.1145/1062745.1062789}
}

@article{liu2009a,
  title = {Mining Data Records in Web Pages},
  language = {English},
  author = {Liu, Bing and Grossman, Robert and Zhai, Y.},
  year = {2009},
  abstract = {A large amount of information on the Web is contained in regularly structured objects, which we call data records. Such data records are important because they often present the essential information of their host pages, e.g., lists of products or services. It is useful to mine such data records in order to extract information from them to provide value-added services. Existing automatic techniques are not satisfactory because of their poor accuracies. In this paper, we propose a more effective technique to perform the task. The technique is based on two observations about data records on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and noncontiguous data records. Our experimental results show that the proposed technique outperforms existing techniques substantially. Categories and Subject Descriptors I.5 [Pattern Recognition]: statistical and structural H.2.8 [Database Applications]: data mining Keywords Web data records, Web mining, Web information integration 1.#}
}


@article{zhai2005a,
  title = {Web data extraction based on partial tree alignment},
  publisher = {ACM USA www.acm.org/publications},
  author = {Zhai, Yanhong and Liu, Bing},
  journal = {International World Wide Web Conference},
  pages = {76-85},
  year = {2005},
  isbn = {1595930469},
  abstract = {This paper studies the problem of extracting data from a Web page that contains several structured data records. The objective is to segment these data records, extract data items/fields from them and put the data in a database table. This problem has been studied by several researchers. However, existing methods still have some serious limitations. The first class of methods is based on machine learning, which requires human labeling of many examples from each Web site that one is interested in extracting data from. The process is time consuming due to the large number of sites and pages on the Web. The second class of algorithms is based on automatic pattern discovery. These methods are either inaccurate or make many assumptions. This paper proposes a new method to perform the task automatically. It consists of two steps, (1) identifying individual data records in a page, and (2) aligning and extracting data items from the identified data records. For step 1, we propose a method based on visual information to segment data records, which is more accurate than existing methods. For step 2, we propose a novel partial alignment technique based on tree matching. Partial alignment means that we align only those data fields in a pair of data records that can be aligned (or matched) with certainty, and make no commitment on the rest of the data fields. This approach enables very accurate alignment of multiple data records. Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately.},
  doi = {10.1145/1060745.1060761}
}

@article{song2009a,
  title = {Efficient record-level wrapper induction},
  publisher = {Association for Computing Machinery},
  author = {Song, Ruihua and Wen, Ji Rong and Giles, C. Lee and Zheng, Shuyi},
  journal = {International Conference on Information and Knowledge Management, Proceedings},
  pages = {47-55},
  year = {2009},
  isbn = {9781605585123},
  abstract = {Web information is often presented in the form of record, e.g., a product record on a shopping website or a personal profile on a social utility website. Given a host webpage and related information needs, how to identify relevant records as well as their internal semantic structures is critical to many online information systems. Wrapper induction is one of the most effective methods for such tasks. However, most traditional wrapper techniques have issues dealing with web records since they are designed to extract information from a page, not a record. We propose a record-level wrapper system. In our system, we use a novel ''broom'' structure to represent both records and generated wrappers. With such representation, our system is able to effectively extract records and identify their internal semantics at the same time. We test our system on 16 real-life websites from four different domains. Experimental results demonstrate 99\% extraction accuracy in terms of F1-Value. Copyright 2009 ACM.},
  doi = {10.1145/1645953.1645962}
}


@article{shasha1990a,
  title = {FAST ALGORITHMS FOR THE UNIT COST EDITING DISTANCE BETWEEN TREES},
  language = {English},
  publisher = {ACADEMIC PRESS INC JNL-COMP SUBSCRIPTIONS},
  author = {Shasha, D. and Zhang, KZ},
  journal = {JOURNAL OF ALGORITHMS},
  volume = {11},
  number = {4},
  pages = {581-621},
  year = {1990},
  issn = {01966774, 10902678},
  abstract = {Ordered labeled trees are trees whose nodes are labeled and in which the left-to-right order among siblings is significant. Given two different trees, a natural question to ask is how similar or different they are. The authors consider the distance between two trees to be the minimum number of edit operations, (insert, delete, and modify) necessary to transform one tree to another. They present three algorithms to find the distance. The first algorithm is a simple dynamic programming algorithm based on a postorder traversal whose complexity improves upon the best previously published algorithm due to Tai (1979). The second and third algorithms are optimal speedup parallel algorithms based on the applications of suffix trees to the comparison problem. The cost of executing these algorithms is a monotonic increasing function of the distance between the two trees.},
  doi = {10.1006/jagm.1993.1036}
}

@article{de2004a,
  title = {Automatic web news extraction using tree edit distance},
  language = {eng},
  author = {De Reis, Davi Castro and Golgher, Paulo B. and Da Silva, Altigran S. and Laender, Alberto H F},
  journal = {Thirteenth International World Wide Web Conference Proceedings, Www2004, Thirteenth Int. World Wide Web Conf. Proc. Www},
  pages = {502-511},
  year = {2004},
  isbn = {158113844x},
  abstract = {The Web poses itself as the largest data repository ever available in the history of humankind. Major efforts have been made in order to provide efficient access to relevant information within this huge repository of data. Although several techniques have been developed to the problem of Web data extraction, their use is still not spread, mostly because of the need for high human intervention and the low quality of the extraction results. In this paper, we present a domain-oriented approach to Web data extraction and discuss its application to automatically extracting news from Web sites. Our approach is based on a highly efficient tree structure analysis that produces very effective results. We have tested our approach with several important Brazilian on-line news sites and achieved very precise results, correctly extracting 87.71% of the news in a set of 4088 pages distributed among 35 different sites.}
}

@article{dalvi2009a,
  title = {Robust web extraction: An approach based on a probabilistic tree-edit model},
  publisher = {Association for Computing Machinery},
  author = {Dalvi, Nilesh and Bohannon, Philip and Sha, Fei},
  journal = {SIGMOD-PODS'09 - Proceedings of the International Conference on Management of Data and 28th Symposium on Principles of Database Systems},
  pages = {335-347},
  year = {2009},
  isbn = {9781605585543},
  abstract = {On script-generated web sites, many documents share com- mon HTML tree structure, allowing wrappers to effectively extract information of interest. Of course, the scripts and thus the tree structure evolve over time, causing wrappers to break repeatedly, and resulting in a high cost of maintaining wrappers. In this paper, we explore a novel approach: we use temporal snapshots of web pages to develop a tree-edit model of HTML, and use this model to improve wrapper construction. We view the changes to the tree structure as suppositions of a series of edit operations: deleting nodes, inserting nodes and substituting labels of nodes. The tree structures evolve by choosing these edit operations stochas- tically. Our model is attractive in that the probability that a source tree has evolved into a target tree can be estimated efficiently-in quadratic time in the size of the trees-making it a potentially useful tool for a variety of tree-evolution problems. We give an algorithm to learn the probabilistic model from training examples consisting of pairs of trees, and apply this algorithm to collections of web-page snap- shots to derive HTML-specific tree edit models. Finally, we describe a novel wrapper-construction framework that takes the tree-edit model into account, and compare the quality of resulting wrappers to that of traditional wrappers on syn- thetic and real HTML document examples. © 2009 ACM.},
  doi = {10.1145/1559845.1559882}
}

@article{zheng2007a,
  title = {Joint Optimization of Wrapper Generation and Template Detection},
  language = {English},
  publisher = {ASSOC COMPUTING MACHINERY},
  author = {Zheng, Shuyi and Wu, Di and Song, Ruihua and Wen, Ji-Rong},
  journal = {KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING},
  pages = {894-902},
  year = {2007},
  isbn = {9781595936097},
  abstract = {Many websites have large collections of pages generated dynamically from an underlying structured source like a database. The data of a category are typically encoded into similar pages by a common script or template. In recent years, some value-added services, such as comparison shopping and vertical search in a specific domain, have motivated the research of extraction technologies with high accuracy. Almost all previous works assume that input pages of a wrapper induction system conform to a common template and they can be easily identified in terms of a common schema of URL. However, we observed that it is hard to distinguish different templates using dynamic URLs today. Moreover, since extraction accuracy heavily depends on how consistent input pages are, we argue that it is risky to determine whether pages share a common template solely based on URLs. Instead, we propose a new approach that utilizes similarity between pages to detect templates. Our approach separates pages with notable inner differences and then generates wrappers, respectively. Experimental results show that our proposed approach is feasible and effective for improving extraction accuracy.}
}

@article{pawlik2011a,
  title = {RTED: A robust algorithm for the tree edit distance},
  language = {eng},
  publisher = {Association for Computing Machinery},
  author = {Pawlik, Mateusz and Augsten, Nikolaus},
  journal = {Proceedings of the Vldb Endowment, Proc. Vldb Endow},
  volume = {5},
  number = {4},
  pages = {334-345},
  year = {2011},
  issn = {21508097},
  abstract = {We consider the classical tree edit distance between ordered labeled trees, which is dened as the minimum-cost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. The main competitors in the eld either have optimal worst-case complexity, but the worst case happens frequently, or they are very ecient for some tree shapes, but degenerate for others. This leads to unpredictable and often infeasible runtimes. There is no obvious way to choose between the algorithms. In this paper we present RTED, a robust tree edit distance algorithm. The asymptotic complexity of RTED is smaller or equal to the complexity of the best competitors for any input instance, i.e., RTED is both ecient and worst-case optimal. We introduce the class of LRH (Left-Right-Heavy) algorithms, which includes RTED and the fastest tree edit distance algorithms presented in literature. We prove that RTED outperforms all previously proposed LRH algorithms in terms of runtime complexity. In our experiments on synthetic and real world data we empirically evaluate our solution and compare it to the state-of-the-art. © 2011 VLDB Endowment.}
}

% vim:wrap linebreak nolist:
